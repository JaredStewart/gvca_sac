{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0dbbe5-0ecd-41f2-b6a7-7e01a81bbaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef643d8a-3ef3-4852-9b48-fe6d2ac50484",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379410d1-5a90-4294-8b85-7c19ad1ca6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_csv('processed/sara.csv', encoding=\"ISO-8859-1\")\n",
    "df_manual.describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958da84-afd9-4a30-9e4d-f611f5c21c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ai = pd.read_csv('processed/processed_survey_data.csv')\n",
    "df_ai.describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca13dc2b-58a9-4b05-8d48-e3a9cd406ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize column names\n",
    "df_manual.columns = df_manual.columns.str.strip()\n",
    "df_ai.columns = df_ai.columns.str.strip()\n",
    "\n",
    "# Rename columns in AI dataset for consistency\n",
    "df_ai.rename(columns={\n",
    "    \"Respondent ID\": \"respondent_id\",\n",
    "    \"Teachers\": \"Teachers\",\n",
    "    \"Concern\": \"Concern\",\n",
    "    \"Communication\": \"Communication\",\n",
    "    \"Good Outcomes\": \"Good Outcomes\",\n",
    "    \"Policies & Administration\": \"Policies/ Administration\",\n",
    "    \"Culture & Virtues\": \"Culture/ Virtues\",\n",
    "    \"Extra-curriculars & Sports\": \"Extra-curriculars/ Sports\",\n",
    "    \"Facilities\": \"Facilities\",\n",
    "    \"Curriculum\": \"Curriculum\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Merge datasets on respondent_id\n",
    "merged = df_manual.merge(df_ai, on=\"respondent_id\", suffixes=(\"_manual\", \"_ai\"))\n",
    "\n",
    "# Identify tag columns\n",
    "tag_columns = [\n",
    "    \"Concern\", \"Curriculum\", \"Good Outcomes\",\n",
    "    \"Policies/ Administration\", \"Teachers\", \"Culture/ Virtues\", \"Communication\", \"Community\",\n",
    "    \"Extra-curriculars/ Sports\", \"Facilities\"\n",
    "]\n",
    "\n",
    "# Find mismatches\n",
    "mismatches = []\n",
    "for tag in tag_columns:\n",
    "    mismatch_rows = merged[(merged[f\"{tag}_manual\"]==\"Yes\") != merged[f\"{tag}_ai\"]]\n",
    "    for _, row in mismatch_rows.iterrows():\n",
    "        mismatches.append({\n",
    "            \"respondent_id\": row[\"respondent_id\"],\n",
    "            \"tag\": tag,\n",
    "            \"manual_value\": row[f\"{tag}_manual\"]==\"Yes\",\n",
    "            \"ai_value\": row[f\"{tag}_ai\"],\n",
    "            \"response\": row[\"response\"] if \"response\" in row else \"N/A\"\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame and display summary\n",
    "mismatch_df = pd.DataFrame(mismatches)\n",
    "print(\"Tag Mismatch Counts:\")\n",
    "print(mismatch_df[\"tag\"].value_counts())\n",
    "mismatch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9b408-7b26-4430-906e-d720f09ed316",
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd265904-0907-4918-98cd-3c233cd64e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_flat = df_manual.melt(\n",
    "    id_vars=[\"respondent_id\"], value_vars=tag_columns, var_name=\"tag\", value_name=\"manual_value\"\n",
    ")\n",
    "manual_flat = manual_flat[~manual_flat[\"manual_value\"].isna()].groupby(\"respondent_id\")[\"tag\"].apply(list).reset_index()\n",
    "manual_flat.rename(columns={\"tag\": \"manual_tags\"}, inplace=True)\n",
    "manual_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40d872-10e4-4ab0-adfb-8a3910b6e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_flat = df_ai.melt(\n",
    "    id_vars=[\"respondent_id\"], value_vars=tag_columns, var_name=\"tag\", value_name=\"ai_value\"\n",
    ")\n",
    "ai_flat = ai_flat[ai_flat[\"ai_value\"]].groupby(\"respondent_id\")[\"tag\"].apply(list).reset_index()\n",
    "ai_flat.rename(columns={\"tag\": \"ai_tags\"}, inplace=True)\n",
    "ai_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf05db-1d12-4524-a8d5-837bf713f583",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    \"respondent_id\", \"manual_tags\", \"ai_tags\", \"common_tags\", \"different_tags\",\n",
    "    \"(Grammar) What makes GVCA a good choice for you and your family?\",\n",
    "    \"(Middle) What makes GVCA a good choice for you and your family?\",\n",
    "    \"(Upper) What makes GVCA a good choice for you and your family?\",\n",
    "    \"(Generic) What makes GVCA a good choice for you and your family?\",\n",
    "    \"(Grammar) Please provide us with examples of how GVCA can better serve you and your family.\",\n",
    "    \"(Middle) Please provide us with examples of how GVCA can better serve you and your family.\",\n",
    "    \"(Upper) Please provide us with examples of how GVCA can better serve you and your family.\",\n",
    "    \"(Generic) Please provide us with examples of how GVCA can better serve you and your family.\"\n",
    "]\n",
    "\n",
    "merged = manual_flat.merge(ai_flat, on=\"respondent_id\", how=\"outer\").fillna(\"[]\")\n",
    "merged[\"manual_tags\"] = merged[\"manual_tags\"].apply(lambda x: set(x))\n",
    "merged[\"ai_tags\"] = merged[\"ai_tags\"].apply(lambda x: set(x))\n",
    "merged[\"common_tags\"] = merged.apply(lambda row: len(row[\"manual_tags\"] & row[\"ai_tags\"]), axis=1)\n",
    "merged[\"different_tags\"] = merged.apply(lambda row: len(row[\"manual_tags\"] ^ row[\"ai_tags\"]), axis=1)\n",
    "merged = merged.merge(df_ai, on=\"respondent_id\", how=\"left\")\n",
    "output = merged[columns_to_keep]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65339c7b-46c1-421e-9203-5e480d41ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['different_tags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6ccee-5c89-4591-a2ae-fdfc4834b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['common_tags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdafc6c3-2136-456f-8d58-98fb80bfd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(output['common_tags'], bins=7, alpha=0.5, label=\"N Tags Common\", color='blue')\n",
    "plt.hist(output['different_tags'], bins=7, alpha=0.5, label=\"N Tags Different\", color='red')\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribution of Tags in Common / Different\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(\n",
    "    f\"artifacts/AI-Manual Tagging Comparison\",\n",
    "    transparent=True,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8050f-ca85-4214-8d2f-4878034b8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('tag_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f82cf-c0af-4a55-b891-f79aefe21183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sets back to lists for proper binary encoding\n",
    "output.loc[:, \"manual_tags\"] = output[\"manual_tags\"].apply(list)\n",
    "output.loc[:, \"ai_tags\"] = output[\"ai_tags\"].apply(list)\n",
    "\n",
    "# Create binary columns for each tag\n",
    "for tag in tag_columns:\n",
    "    output.loc[:, f\"manual_{tag}\"] = output[\"manual_tags\"].apply(lambda tags: tag in tags)\n",
    "    output.loc[:, f\"ai_{tag}\"] = output[\"ai_tags\"].apply(lambda tags: tag in tags)\n",
    "\n",
    "# Compute correlation between AI and manual labels\n",
    "correlation_results = {tag: output[f\"manual_{tag}\"].corr(output[f\"ai_{tag}\"]) for tag in tag_columns}\n",
    "\n",
    "# Convert correlation results to a DataFrame\n",
    "correlation_df = pd.DataFrame.from_dict(correlation_results, orient='index', columns=['correlation']).reset_index()\n",
    "correlation_df.rename(columns={'index': 'tag'}, inplace=True)\n",
    "correlation_df = correlation_df.sort_values(by='correlation', ascending=False)\n",
    "correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42045dd0-43bb-4ff7-8ae2-bc07d0642f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrices for each tag\n",
    "confusion_matrices = {}\n",
    "for tag in tag_columns:\n",
    "    y_true = output[f\"manual_{tag}\"].astype(int)\n",
    "    y_pred = output[f\"ai_{tag}\"].astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "    print(tag, f1, precision, recall)\n",
    "    print(cm)\n",
    "    print()\n",
    "    # confusion_matrices[tag] = cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
